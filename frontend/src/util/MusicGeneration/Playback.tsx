
export const Playback = () => {

}

// // This file houses all necessary data/functions/variables to play back individual MIDI notes [using playMidiNote()] or entire MIDI files [using playMidiFile()]
// // Special thanks to Dr. Leinecker and Dr. Ayers, this file houses a lot of data and functionality from their "microtonality" project, found at microtonality.net.

// import MidiPlayer from 'midi-player-js';
// import * as Constants from '../Constants.js';
// import * as Overtones from './InstOvertoneDefinitions';
// import {
//     getNoteLengthStringFromInt,
//     getIntFromNoteTypeString,
// 	getMillisecondsFromBPM,
// 	findNumSamples,
//     getFrequencyFromNoteOctaveString
// } from './MusicHelperFunctions.js';

// // ------------------------------------------------------------------------------ GLOBAL VARIABLES ------------------------------------------------------------------------------

// export class Playback {
//     // var instrumentArr = [-1, 0, 1, 2];
//     // var noteTypeArr = [1, 2, 3, 4];

//     // This has to be global I tried I'm sorry
//     var instrList:Array<number>;

//     /*  Essentially, all of the AudioContexts (which actually allow for audio playback in the browser) get placed into this queue.
//         Whenever any single note is played, a new AudioContext is generated to avoid any collisions with other sounds, since we will
//         be having multiple instruments playing simultaneously and having them in one or few AudioContexts could cause issues.
//         Chrome has a limit of 50 AudioContexts that can exist at once, so this program is set so when the queue reaches a size of 45, 
//         the oldest AudioContext gets closed and disconnected (essentially stopping all of its operations to avoid memory leaks) and 
//         then is removed from the queue, moving the next oldest AudioContext to the front of the line to be killed off next.
//         This theoretically should mean that we are never going over Chrome's limit, so our audio should never cut off even if the
//         program is running for a large amount of time, while also ensuring that AudioContexts will exist for as long as possible so
//         that we can minimize the chance of cutting off notes that are still playing.
//         queueOfAudio and killOldestAudioContextIfNecessary() were NOT taken from Microtonality.net and is a custom way of handling the 
//         removal of old AudioContexts. It is in the Microtonality.net section of this file for ease-of-reading and because the 
//         Microtonality.net functions had to be tweaked to work with this new queue system. 								              */
//     var queueOfAudio = new Array();

// 	constructor() {
//         instrList
// 	}

//     // Total number of AudioContexts that have been generated by the program. Each AudioContext is used for one note and only one note.
//     var numContexts = 0;

//     //These values will determine all instrument volume which is changed in GUI
//     var volumeDeltas = [0, 0, 0, 0];

//     var allowToPlay = true;

    
        
//     // ------------------------------------------------------------------------------ MAIN FUNCTIONS ------------------------------------------------------------------------------

//     export function playMidiFile(uri:string, instArr:Array<number>, NTArr:Array<number>, bpm:number)
//     {
//         if (!allowToPlay)
//             return;
            
//         allowToPlay = false;

//         // Initialize player and register event handler
//         setupInstrumentList();
//         var Player = new MidiPlayer.Player(function(event) {});
        
//         console.log("URI passed in: " + uri);
//         Player.loadDataUri(uri);
//         Player.play();

//         var eventCount = 0;

//         // Event listener for midiEvents
//         Player.on('midiEvent', function(event) {
//             if (event.noteNumber != 0 && event.noteNumber != undefined && event.name !== "Note off") //eventCount % 2 == 0) // As long as the midiEvent is a valid NOTE (other events not supported) and not undefined
//             {
//                 console.log("" + eventCount + ": " + event.noteNumber + ", AKA " + event.noteName + " [Track " + event.track + "]");
//                 //console.log(event)

//                 var midiNoteFrequency = getFrequencyFromNoteOctaveString(event.noteName);

//                 playMidiNote(midiNoteFrequency, Constants.DEFAULT_VOLUME + volumeDeltas[event.track - 1], instArr[event.track - 1], getNoteLengthStringFromInt(NTArr[event.track - 1]));
//             }
//             eventCount++;
//         });

//         Player.on('endOfFile', function() {
//             allowToPlay = true;
//         });
//     }

//     // The amount of time (in milliseconds) that each of the supported notes would take at the specified BPM.
//     const timeForEachNoteARRAY =
//     [
//         getMillisecondsFromBPM(BPM) / 4,
//         getMillisecondsFromBPM(BPM) / 2,
//         getMillisecondsFromBPM(BPM),
//         getMillisecondsFromBPM(BPM) * 2,
//         getMillisecondsFromBPM(BPM) * 4
//     ];

//     function killOldestAudioContextIfNecessary() {
//         // If the number of existing AudioContexts in the queue is >= 45, kill off the oldest AudioContext completely and shift the queue accordingly.
//         if (numContexts >= 45) {
//             queueOfAudio[0].ctx.close();
//             queueOfAudio[0].node.disconnect();
//             queueOfAudio.shift();
//             numContexts--; // Decrement the numContexts variable because we removed one from the queue
//         }
//     }

//     // Places all instrument frequency/overtone data into one massive array
//     function setupInstrumentList() {
//         instrList = [];
//         instrList.push(flute);
//         instrList.push(oboe);
//         instrList.push(clarinet);
//         instrList.push(bassoon);
//         instrList.push(trumpet);
//         instrList.push(frenchhorn);
//         instrList.push(trombone);
//         instrList.push(tuba);
//     }

//     // The function that plays audio!
//     // Frequency is the raw frequency of the note you want to play, in hz. [float]
//     //     - See getFrequencyFromNoteOctaveString function in HelperFunctions.js for help with converting.
//     // Amplitude is volume control [float]
//     // SoundType is the instrument being used to play the note [int]
//     // NoteLength is whether the note is sixteenth, eighth, quarter, half, or whole [string]
//     export function playMidiNote(frequency, amplitude, soundType, noteLength) {
//         killOldestAudioContextIfNecessary();

//         queueOfAudio.push({ freq: frequency, playing: false, ctx: 0, buffer: 0, node: 0, gain: 0, needToClose: false, number: numContexts });

//         //console.log("Number of current contexts: " + numContexts + ", array: " + queueOfAudio);

//         if (queueOfAudio[numContexts].playing)
//             return false;

//         queueOfAudio[numContexts].playing = true;
//         queueOfAudio[numContexts].needToClose = false;

//         queueOfAudio[numContexts].ctx = new AudioContext();
//         queueOfAudio[numContexts].buffer = getNoteData(soundType, queueOfAudio[numContexts].freq, amplitude, queueOfAudio[numContexts].ctx, noteLength);
//         queueOfAudio[numContexts].node = queueOfAudio[numContexts].ctx.createBufferSource();
//         queueOfAudio[numContexts].node.buffer = queueOfAudio[numContexts].buffer;

//         // We need this gain object so that at the end of the note play we can taper the sound.
//         queueOfAudio[numContexts].gain = queueOfAudio[numContexts].ctx.createGain();
//         queueOfAudio[numContexts].node.connect(queueOfAudio[numContexts].gain);
//         queueOfAudio[numContexts].gain.connect(queueOfAudio[numContexts].ctx.destination);
//         queueOfAudio[numContexts].gain.gain.value = amplitude;

//         // Set to loop, although there is sill a perceptable break at the end.
//         queueOfAudio[numContexts].node.loop = false;

//         // Start the note.
//         // This needs to be edited; the third argument of the function will only ever make quarter notes.
//         queueOfAudio[numContexts].node.start(0, 0, getMillisecondsFromBPM(BPM) / 1000);

//         // Increment the numContexts variable to reflect the new AudioContext added to the queue.
//         numContexts++;

//         return true;
//     }

//     function getNoteData(soundType:number, freq:number, amplitude:number, ctx:any, noteLength:number) {
//         var buffer; // Local buffer variable.

//         // For each supported sound type we call the correct function.
//         if (soundType == Constants.instrumentEnums.SineWave)
//             buffer = sineWave(findNumSamples(timeForEachNoteARRAY[getIntFromNoteTypeString(noteLength)]), freq, amplitude, ctx);
//         else if (soundType == Constants.instrumentEnums.TriangleWave)
//             buffer = triangleWave(findNumSamples(timeForEachNoteARRAY[getIntFromNoteTypeString(noteLength)]), freq, amplitude, ctx);
//         else if (soundType == Constants.instrumentEnums.SquareWave)
//             buffer = squareWave(findNumSamples(timeForEachNoteARRAY[getIntFromNoteTypeString(noteLength)]), freq, amplitude, ctx);
//         else
//             buffer = instrumentWave(findNumSamples(timeForEachNoteARRAY[getIntFromNoteTypeString(noteLength)]), freq, ctx, soundType);

//         return buffer;
//     }

//     // This function used to be called "getAmplitude", which I believe to be incorrect; it does math to determine overtone FREQUENCIES, not amplitudes.
//     function getOvertoneFrequencies(instrumentIndex, frequency) {
//         // Get the list of note amplitude values for this instrument.
//         let list = instrList[instrumentIndex];

//         // We will start with a default value.
//         let index = 0;
//         //console.log("frequency : " + frequency, ", list: " + list + ", instrumentIndex: " + instrumentIndex);
//         let diff = Math.abs(frequency - list[0][0]);

//         // Loop through the list of frequencies/amplitudes and find the closest match.
//         for (let i = 1; i < list.length; i++) {
//             // Get the difference between incoming frequency value and the frequeny of this list element.
//             let td = Math.abs(frequency - list[i][0]);

//             // If this is less (we are closer to the specified frequency) then we record the index and remember the new difference.
//             if (td < diff) {
//                 diff = td;
//                 index = i;
//             }
//         }

//         // Here we take the current array and make a new array to return.
//         let retList = [];
//         for (let i = 1; i < list[index].length; i++) {
//             retList.push(i); // Push the harmonic number.
//             retList.push(list[index][i]); // Push the amplitude.
//         }

//         return retList;
//     }

//     // ------------------------------------------------------------------------------ INSTRUMENT-SPECIFIC FUNCTIONS AND DATA ------------------------------------------------------------------------------


//     // Sine wave
//     function sineWave(numSamples, frequency, amplitude, ctx) {
//         // Precalculate 2PI
//         let PI_2 = Math.PI * 2;

//         // Create the buffer for the node.
//         let buffer = ctx.createBuffer(1, numSamples, Constants.sampleRate);

//         // Create the buffer into which the audio data will be placed.
//         let buf = buffer.getChannelData(0);

//         // Loop numSamples times -- that's how many samples we will calculate and store.
//         for (let i = 0; i < numSamples; i++) {
//             // Calculate and store the value for this sample.
//             buf[i] = Math.sin(frequency * PI_2 * i / Constants.sampleRate) * amplitude;
//         }

//         // Return the channel buffer.
//         return buffer;
//     }

//     // Triangle wave
//     function triangleWave(numSamples, frequency, amplitude, ctx) {

//         // Here we calculate the number of samples for each wave oscillation.
//         var samplesPerOscillation = Constants.sampleRate / frequency;
//         // This is the first quarter of the oscillation. 0 - 1/4
//         var first = samplesPerOscillation / 4;
//         // This is the second quarter of the oscillation. 1/4 - 1/2
//         var second = samplesPerOscillation / 2;
//         // This is the third quarter of the oscillation. 1/2 - 3/4
//         var third = (samplesPerOscillation / 2) + (samplesPerOscillation / 4);
//         // We will count the samples as we go.
//         var counter = 0;

//         // Step value. This is how much the sample value changes per sample.
//         var step = 1 / first;

//         // Create the buffer for the node.
//         var buffer = ctx.createBuffer(1, numSamples, Constants.sampleRate);

//         // Create the buffer into which the audio data will be placed.
//         var buf = buffer.getChannelData(0);

//         // Loop numSamples times -- that's how many samples we will calculate and store.
//         for (var i = 0; i < numSamples; i++) {
//             // Increment the counter.
//             counter++;

//             // See if this is the first quarter.
//             if (counter <= first) {
//                 // Store the value.
//                 buf[i] = step * counter * amplitude;
//             }
//             // See if this is the second quarter.
//             else if (counter <= second) {
//                 // We want the count relative to this quarter.
//                 var cnt = counter - first;

//                 // Store the value.
//                 buf[i] = 1 - step * cnt * amplitude;
//             }
//             // See if this is the third quarter.
//             else if (counter <= third) {
//                 // We want the count relative to this quarter.
//                 var cnt = counter - second;

//                 // Store the value.
//                 buf[i] = -(step * cnt) * amplitude;
//             }
//             // This is the fourth quarter.
//             else {
//                 // We want the count relative to this quarter.
//                 var cnt = counter - third;

//                 // Store the value.
//                 buf[i] = -1 + (step * cnt) * amplitude;

//                 // See if we are done with this cycle.
//                 if (counter >= samplesPerOscillation) {
//                     // Set to zero so we are ready for another cycle.
//                     counter = 0;
//                 }
//             }
//         }
//         return buffer;
//     }

//     // Square wave
//     function squareWave(numSamples:number, frequency:number, amplitude:number, ctx:any) {

//         // Here we calculate the number of samples for each wave oscillation.
//         var samplesPerOscillation = Constants.sampleRate / frequency;
//         // Create the value for the first oscillation change.
//         var first = samplesPerOscillation / 2;
//         // We will count the samples as we go.
//         var counter = 0;

//         // Create the buffer for the node.
//         var buffer = ctx.createBuffer(1, numSamples, Constants.sampleRate);

//         // Create the buffer into which the audio data will be placed.
//         var buf = buffer.getChannelData(0);

//         // Loop numSamples times -- that's how many samples we will calculate and store.
//         for (var i = 0; i < numSamples; i++) {
//             // Increment the counter.
//             counter++;

//             // This is the first half of the oscillation. it should be 1.
//             if (counter <= first) {
//                 // Store the value.
//                 buf[i] = 1 * amplitude;
//             }
//             // This is the second half of the oscillation. It should be -1.
//             else {
//                 // Store the value.
//                 buf[i] = -1 * amplitude;

//                 // See if we are done with this cycle.
//                 if (counter >= samplesPerOscillation) {
//                     // Set to zero so we are ready for another cycle.
//                     counter = 0;
//                 }
//             }
//         }

//         // Return the channel buffer.
//         return buffer;
//     }

//     // "Real" instrument additive synthesis
//     function instrumentWave(numSamples:number, frequency:number, ctx:any, soundType) {
//         setupInstrumentList();

//         // Get the instrument specs.
//         let inst = getOvertoneFrequencies(soundType, frequency);

//         // Precalculate 2PI
//         let PI_2 = Math.PI * 2;

//         // Create the buffer for the node.
//         let buffer = ctx.createBuffer(1, numSamples, Constants.sampleRate);

//         // Create the buffer into which the audio data will be placed.
//         var buf = buffer.getChannelData(0);

//         // Zero the buffer
//         for (var i = 0; i < numSamples; i++) {
//             buf[i] = 0;
//         }

//         // Loop through the instrument spec.
//         for (var j = 0; j < inst.length / 2; j++) {
//             // Get the frequency multiplier from the data array.
//             var f = frequency * inst[j * 2];
//             //console.log("f: ", f, ", which is ", frequency, " times ", inst[j*2])
//             // Get the amplitude value from the data array.
//             var a = inst[j * 2 + 1];
//             //console.log("a: ", a)
//             // Loop numSamples times -- that's how many samples we will calculate and store.
//             for (var i = 0; i < numSamples; i++) {
//                 // Calculate and store the value for this sample.
//                 buf[i] += (Math.sin(f * PI_2 * i / Constants.sampleRate) * a);
//                 //buf[i] = frequency;
//             }
//         }

//         // Return the channel buffer.
//         return buffer;
//     }
// }